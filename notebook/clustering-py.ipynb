{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os  # <--- Get environmental variables\n",
    "import re  # <--- Regex for data cleaning\n",
    "import json  # <--- Loading JSON into Python objects\n",
    "import dirtyjson  # <--- backup method for loading dirty JSON into Python objects\n",
    "import asyncio  # <--- Execute tasks asynchronously\n",
    "import pandas as pd  # <--- DataFrame usage for dataset operations\n",
    "import numpy as np  # <--- Numpy arrays for clustering algorithms\n",
    "import plotly.express as px  # <--- Fancy graphing!\n",
    "from getpass import getpass  # <--- Get OpenAI API key if not stored in environmental variable\n",
    "from helpers import DocumentTransformer, EvtxHandler, Filter  # <--- Custom funcs/classes for parsing/transforming EVTX data\n",
    "from openai import AsyncOpenAI  # <--- Async OpenAI client\n",
    "from sklearn.cluster import DBSCAN  # <--- Our clustering algorithm\n",
    "from sklearn.decomposition import PCA  # <--- Princaple Component Analysis for graphing\n",
    "from typing import List, Tuple  # <--- Typing to help convey variable types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get user input\n",
    "User will need to specify OpenAI key and a source of event logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OpenAI API key from env var or else prompt for it.\n",
    "openai_key = os.environ.get(\"OPENAI_KEY\", None) or getpass(\"Enter your OpenAI key:\")\n",
    "\n",
    "# We are going to pull commands from EVTX files. Provide a path to EVTX files that contain Sysmon logs.\n",
    "evtx_source = input(\"Enter a source for a EVTX file or folder that contains EVTX files: \")\n",
    "print(f\"EVTX Source: {evtx_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a EvtxHandler that can transform and filter events\n",
    "These are helper classes in the helpers.py module that sits in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to transform EVTX records into rows that only have a couple columns.\n",
    "document_transformer = DocumentTransformer.from_fields([\n",
    "        (\"Timestamp\", \"Event.System.TimeCreated.\\\"#attributes\\\".SystemTime\"),\n",
    "        (\"Computer\", \"Event.System.Computer\"),\n",
    "        (\"Provider\", \"Event.System.Provider.\\\"#attributes\\\".Name\"),\n",
    "        (\"EventID\", \"Event.System.EventID.\\\"#text\\\"||Event.System.EventID\"),\n",
    "        (\"CommandLine\", \"Event.EventData.CommandLine\"),\n",
    "    ])\n",
    "\n",
    "# We only want to return EVTX records that have Event.EventData.CommandLine populated.\n",
    "evtx_filter = Filter.from_pattern(\"Event.EventData.CommandLine\")\n",
    "\n",
    "# Create an EvtxHandler to make EVTX operations easy.\n",
    "evtx_handler = EvtxHandler.from_source(evtx_source)\\\n",
    "    .with_transformer(document_transformer)\\\n",
    "    .with_filter(evtx_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse EventLogs into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a DataFrame that represents our EVTX data.\n",
    "dataframe = evtx_handler.parse_into_dataframe()\n",
    "# Show first five records\n",
    "dataframe.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Events: {}\".format(dataframe.shape[0]))\n",
    "print(\"Unique commands found: {}\".format(\n",
    "    dataframe[\"CommandLine\"].unique().shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Embeddings from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create async OpenAI client\n",
    "client = AsyncOpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an async function to fetch embeddings for given text\n",
    "async def get_embedding(\n",
    "    text: str, semaphore: asyncio.Semaphore, \n",
    "    model=\"text-embedding-3-small\", dimensions=None\n",
    ") -> Tuple[str, List[float]]:\n",
    "    # Use a Semaphore to keep a max number to throttle requests\n",
    "    async with semaphore as sep:\n",
    "        # Request dimensions if provided, otherwise send without dimensions param\n",
    "        if dimensions:\n",
    "            response = await client.embeddings.create(input=text, model=model, dimensions=dimensions)\n",
    "        else:\n",
    "            response = await client.embeddings.create(input=text, model=model)\n",
    "        # Extract the embedding vector\n",
    "        embedding = response.data[0].embedding\n",
    "        # Return the command and it's embedding vector\n",
    "        return text, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate Commands Requesting the Embeddings for Each One\n",
    "Remember we only want to iterate on **unique** CommandLines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Semaphore to limit how many requests can happen at a time\n",
    "semaphore = asyncio.Semaphore(25)\n",
    "# Create async tasks to generate embeddings for CommandLines\n",
    "tasks = [get_embedding(cmd, semaphore) for cmd in dataframe[\"CommandLine\"].unique()]\n",
    "# Get the results of our embedding generation\n",
    "embedding_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the embeddings and commands\n",
    "df_embeddings = pd.DataFrame(embedding_results, columns=[\"cmd\", \"embedding_vector\"])\n",
    "# Show five records\n",
    "df_embeddings.iloc[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_embeddings.iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cluster data we need to break the embeddings into an array\n",
    "unique_command_lines_vectors = np.array(list(df_embeddings[\"embedding_vector\"]))\n",
    "# Collect the list of commands\n",
    "command_list = df_embeddings[\"cmd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the DBSCAN clustering algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the command line vectors\n",
    "dbscan = DBSCAN(n_jobs=-1, min_samples=2, eps=0.56)\n",
    "dbscan.fit(unique_command_lines_vectors)\n",
    "print(\"Number of Clusters: {}\".format(len(set(dbscan.labels_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that contains our commands and their associated clusters\n",
    "clusted_commands_df = pd.DataFrame({\"Command\": command_list, \"Cluster\": dbscan.labels_})\n",
    "# View cluster counts out of curiosity\n",
    "clusted_commands_df.groupby([\"Cluster\"]).agg(\"count\").reset_index()\\\n",
    "    .rename(columns={\"Command\": \"Unique Commands in Cluster\"}).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Commands in a Given Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is cluster 79?\n",
    "clusted_commands_df[clusted_commands_df[\"Cluster\"]==79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert multi-dimensional vectors into 3 dimensions for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Princaple Component Analysis to our multi dimensional vectors to simplify down to 3d vectors for plotting\n",
    "pca = PCA(3, n_oversamples=1)\n",
    "pca.fit(unique_command_lines_vectors)\n",
    "three_dimensions = pca.transform(unique_command_lines_vectors)\n",
    "print(\"Example: {} => {}\".format(unique_command_lines_vectors[0], three_dimensions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing the Cluster into 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will create a scatter plot figure of vectors in 3 dimensions\n",
    "def scatter3d(data: List[Tuple[float, float, float]], labels: List[int], exclude_unclustered=False):\n",
    "    \"\"\"Data is a list of 3d vectors. Labels are the clusters that correlate to the a given vector.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame that has the X, Y, Z coordinates of each item.\n",
    "    _df = pd.DataFrame({\n",
    "        \"cluster\": labels,\n",
    "        \"x\": data[:, 0],\n",
    "        \"y\": data[:, 1],\n",
    "        \"z\": data[:, 2]\n",
    "    })\n",
    "    # Exclude unclusted data if requested\n",
    "    if exclude_unclustered:\n",
    "        _df = _df[_df[\"cluster\"] != -1]\n",
    "\n",
    "    # Create a figure with our DataFrame\n",
    "    fig = px.scatter_3d(\n",
    "        _df,\n",
    "        x='x', y='y', z='z',\n",
    "        color='cluster'\n",
    "    )\n",
    "    fig.write_html(\"plot.html\")\n",
    "    # Return the figure\n",
    "    return fig\n",
    "\n",
    "# Plot the data\n",
    "scatter3d(three_dimensions, dbscan.labels_, exclude_unclustered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View an example of a cluster\n",
    "clusted_commands_df[clusted_commands_df[\"Cluster\"]==30].iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the LLM to Assess Risk of Each Cluster\n",
    "### Define Prompts\n",
    "The better the prompt, the better your results will be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of samples from each cluster to analyze\n",
    "command_sample_size = 10\n",
    "# System prompt for each OpenAI chat request\n",
    "system_prompt = r\"\"\"You are a digital forensics and incident response analyst reviewing commands executed on a Windows system. \n",
    "You are also proficent with data science and understand machine learning strategies. \n",
    "You are using the DBSCAN clusterning algorithm to group commands executed. For each cluster, assess the risk of the commands used.\n",
    "Commands are a sample of the given cluster. The command will be between the following tags:  <command> and </command>\"\"\"\n",
    "# Chat prompt for each cluster\n",
    "user_prompt_template = \"Analyze the following commands given this sample of cluster {cluster_number}.\\n\\nYour output must be \" \\\n",
    "\"valid JSON that adheres to the JSON standard with the following format.\\n\\nrisk_score must be a value between 0-10.\" \\\n",
    "\"\\n\\n<output format>\\n{{\\n\\t\\\"risk_score\\\": <int>\\n\\t\\\"cluster_description\\\": <str>\\n}}\\n</output format>\\n\\n\" \\\n",
    "\"<commands to analyze>\\n{command}\\n</commands to analyze>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the LLM to Assess Risk of Each Cluster\n",
    "### Function to send OpenAI requests using prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that prompts GPT to summarize and provide a risk summary for each command cluster\n",
    "async def collect_risk_summaries(command_sample_size: int, system_prompt: str, user_prompt_template: str):\n",
    "    # Rank the clusters by severity\n",
    "    responses = []\n",
    "    # Iterate each cluster of commands\n",
    "    for cluster_number in clusted_commands_df[\"Cluster\"].unique():\n",
    "        if cluster_number == -1:\n",
    "            # Skip unclustered data for now\n",
    "            continue\n",
    "        \n",
    "        # Fetch the commands for just the current cluster\n",
    "        _this_cluster = clusted_commands_df[clusted_commands_df[\"Cluster\"] == cluster_number]\n",
    "        # Grab a sample of the cluster (or all if less than sample size)\n",
    "        _this_cluster_sample = _this_cluster if _this_cluster.shape[0] <= command_sample_size \\\n",
    "            else _this_cluster.sample(command_sample_size)\n",
    "\n",
    "        # Create a string of all the sample commands to insert into the user prompt\n",
    "        _cmd_list = [\"\\n\".join([f\"<command>\", cmd, f\"</command>\"]) for cmd in _this_cluster_sample[\"Command\"]]\n",
    "        cmd_body = \"\\n\\n\".join(_cmd_list)\n",
    "\n",
    "        # Craft the user prompt for this cluster of commands\n",
    "        user_prompt = user_prompt_template.format(cluster_number=cluster_number, command=cmd_body)\n",
    "\n",
    "        # Get the response and added it to the responses to be returned\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # <--- The OpenAI chat model to use\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # <--- System prompt\n",
    "                {\"role\": \"user\", \"content\": user_prompt}  # <--- User prompt\n",
    "            ]\n",
    "        )\n",
    "        responses.append(response)\n",
    "    # Return OpenAI Chat response\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a cleanup function for parsing JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to parse json data from a response (sometimes a little cleaning is required)\n",
    "def parse_json(opanai_response):\n",
    "    # Fetch the content from the openai response\n",
    "    content = opanai_response.choices[0].message.content\n",
    "    # Search for a json response in the content\n",
    "    json_str = re.search(r'(\\{.*\\})', content, re.DOTALL).group(1)\n",
    "    # Strip single slashes in body (this is a common issue)\n",
    "    json_str = re.sub(r'(?<!\\\\)\\\\(?!\\\\)', \"\\\\\\\\\\\\\\\\\", json_str)\n",
    "    # Attempt to parse the json response\n",
    "    data = None\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        try:\n",
    "            data = dirtyjson.loads(json_str)\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        raise Exception(\"Unable to parse a JSON response.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Risk Responses into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the risk responses from GPT\n",
    "risk_summary_responses = await collect_risk_summaries(\n",
    "    command_sample_size, system_prompt, user_prompt_template\n",
    ")\n",
    "# Create a DataFrame of the responses\n",
    "rankings_df = pd.DataFrame([parse_json(r) for r in risk_summary_responses])\n",
    "# We skipped unclustered data (-1) so start at index 1\n",
    "rankings_df[\"cluster_number\"] = clusted_commands_df[\"Cluster\"].unique()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_df[rankings_df[\"cluster_number\"]==79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Excel Workbook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('command_clusters.xlsx', engine='xlsxwriter')   \n",
    "workbook=writer.book\n",
    "\n",
    "_worksheet_r=workbook.add_worksheet('Cluster Rankings')\n",
    "writer.sheets['Cluster Rankings'] = _worksheet_r\n",
    "rankings_df.to_excel(\n",
    "    writer, sheet_name='Cluster Rankings', \n",
    "    columns=[\"cluster_number\", \"risk_score\", \"cluster_description\"], \n",
    "    startrow=0 , startcol=0, index=False\n",
    ")\n",
    "\n",
    "_worksheet_c=workbook.add_worksheet('Commands')\n",
    "writer.sheets['Commands'] = _worksheet_c\n",
    "clusted_commands_df.to_excel(\n",
    "    writer, sheet_name='Commands', \n",
    "    columns=[\"Cluster\", \"Command\"], \n",
    "    startrow=0 , startcol=0, index=False\n",
    ")\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
